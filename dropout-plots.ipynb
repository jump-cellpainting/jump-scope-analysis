{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load checkpoints\n",
    "\n",
    "# df_replicating = pd.read_csv(\"checkpoints/moa-replicating-sphering.csv\", converters={'Null_Replicating':utilssphering.safe_literal_eval, \"Replicating\":utilssphering.safe_literal_eval)\n",
    "# df_matching = pd.read_csv(\"checkpoints/moa-matching-sphering.csv\", converters={'Null_Matching':utilssphering.safe_literal_eval, \"Matching\":utilssphering.safe_literal_eval)\n",
    "\n",
    "# For some reason, adding cell counts makes the above evals fail with an \"SyntaxError: unexpected EOF while parsing\"\n",
    "df_replicating = pd.read_csv(\"checkpoints/moa-replicating-sphering.csv\")\n",
    "df_matching = pd.read_csv(\"checkpoints/moa-matching-sphering.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Creating dropout datasets + matrix\n",
    "\n",
    "def return_prop95(dataframe, match_or_rep=\"replicating\", enable_sphering=False):\n",
    "    \"\"\"\n",
    "    dataframe: pandas.DataFrame to find the prop95 for \n",
    "    \"\"\"\n",
    "    n_samples = 1000\n",
    "    n_replicates = 4  #number of sample replicates within each plate \n",
    "    metadata_common = 'Metadata_moa'\n",
    "    # metadata_perturbation = 'Metadata_broad_sample'\n",
    "    # group_by_feature = 'Metadata_broad_sample'\n",
    "    metadata_perturbation = 'Metadata_pert_iname'\n",
    "    group_by_feature = 'Metadata_pert_iname'\n",
    "    if match_or_rep.casefold() == \"replicating\":\n",
    "        if enable_sphering:\n",
    "            _, _, prop_95_replicating_sphere, _ = utilssphering.calculate_percent_replicating_MOA(\"\", \"\", data_df=dataframe)\n",
    "            return prop_95_replicating_sphere\n",
    "\n",
    "        if not enable_sphering: \n",
    "            plate_df = utils.remove_negcon_empty_wells(dataframe)\n",
    "            replicate_corr = list(utils.corr_between_replicates(plate_df, group_by_feature))\n",
    "            null_replicating = list(utils.corr_between_non_replicates(plate_df, n_samples=n_samples, n_replicates=n_replicates, metadata_compound_name=group_by_feature))\n",
    "            prop_95_replicating, _ = utils.percent_score(null_replicating, replicate_corr, how='right')\n",
    "            return prop_95_replicating\n",
    "\n",
    "    if match_or_rep.casefold() == \"matching\":\n",
    "        if enable_sphering:\n",
    "            _, _, prop_95_matching_sphere, _ = utilssphering.calculate_percent_matching_MOA(\"\", \"\", data_df=dataframe)\n",
    "            return prop_95_matching_sphere\n",
    "\n",
    "        if not enable_sphering:\n",
    "            plate_df = utils.remove_negcon_empty_wells(dataframe)\n",
    "            matching_corr = list(utils.corr_between_perturbation_pairs(plate_df, 'Metadata_moa', 'Metadata_broad_sample'))\n",
    "            null_matching = list(utils.corr_between_perturbation_non_pairs(plate_df, n_samples=n_samples, metadata_common=metadata_common, metadata_perturbation=metadata_perturbation))\n",
    "            prop_95_matching, _ = utils.percent_score(null_matching, matching_corr, how='right')\n",
    "            return prop_95_matching\n",
    "\n",
    "\n",
    "def do_feature_select(plate_df):\n",
    "    \"\"\"\n",
    "    Find the column names that are CellProfiler features. Eg. column names\n",
    "    that start with \"Nuclei\" or \"Cytoplasm\"\n",
    "    \"\"\"\n",
    "    feature_select_features = pycytominer.cyto_utils.infer_cp_features(\n",
    "        plate_df\n",
    "    )\n",
    "    # For all of the cellprofiler features, perform these operations on them\n",
    "    return pycytominer.feature_select(\n",
    "        profiles=plate_df,\n",
    "        features=feature_select_features,\n",
    "        operation=['variance_threshold','correlation_threshold',\n",
    "        'drop_na_columns','blocklist']\n",
    "    )\n",
    "\n",
    "def all_combo_dropouts_in_a_category(category_list,outfile,subsample=None):\n",
    "    \"\"\"\n",
    "    Create a database with dropped out features\n",
    "\n",
    "    category_list: the categories to drop out (for example, Nuclei, Cytoplasm, Cells)\n",
    "    outfile: save csv path\n",
    "    \"\"\"\n",
    "    # For all of the categories, find all possible combinations with itertools.combinations\n",
    "    to_try = [y for x in range(len(category_list)+1) for y in list(set(itertools.combinations(category_list,x)))]\n",
    "\n",
    "    # If the output csv already exists, check what dropouts have already been tried\n",
    "    # Don't try them again\n",
    "    if os.path.exists(outfile):\n",
    "        dropout_df = pd.read_csv(outfile)\n",
    "        tried=list(dropout_df['Dropout'])\n",
    "        print(f\"Already try {tried},skipping\")\n",
    "        to_try = [x for x in to_try if str(x) not in tried]\n",
    "    # Otherwise, dropout columns will be generated from scratch\n",
    "    else:\n",
    "        dropout_df = pd.DataFrame()\n",
    "    \n",
    "    # For the given categories, go through each combination (eg. ('Cells', 'Cytoplasm'), ('Nuclei', 'Cytoplasm') etc.)\n",
    "    for eachdropout in to_try:\n",
    "        # Get what columns are expected. I don't like this method\n",
    "        normalized_master = pd.read_csv('../profiles-pilots/profiles/Stain5_CondC_Standard/BR00120274/BR00120274_normalized_negcon.csv.gz')\n",
    "        col_list = normalized_master.columns\n",
    "        if subsample:\n",
    "             for each_item in subsample:\n",
    "                metadata_list = [x for x in col_list if 'Metadata' in x] \n",
    "                col_list = metadata_list+[x for x in col_list if each_item in x]\n",
    "        for each_item in eachdropout:\n",
    "            # Create a list of columns that DOESN'T contain the categories being iterated\n",
    "            # Thus this will iterate over all categories (eg. nuclei, cell, cytoplasm) and\n",
    "            # drop all of the column names\n",
    "            col_list = [x for x in col_list if each_item not in x]\n",
    "        dropout_dict = {}\n",
    "        temp_dropout_dict = {}\n",
    "        # Code unique to this dataset. Iterate through plates\n",
    "        for eachplate in range(70,78):\n",
    "            try:\n",
    "                # Read only the columns previously defined for each plate being iterated over\n",
    "                # Again, don't like this hardcoding. Should be some sort of os.walk situation\n",
    "                normalized = pandas.read_csv(\n",
    "                    f'../profiles-pilots/profiles/Stain5_CondC_Standard/BR001202{eachplate}/BR001202{eachplate}_normalized_negcon.csv.gz',\n",
    "                    usecols=col_list)\n",
    "                # For the loaded dataset, calculate feature selection AFTER column dropout\n",
    "                feature_select = do_feature_select(normalized)\n",
    "                temp_dropout_dict[f'{eachplate}_featnum']=len(feature_select.columns)\n",
    "                # Now, calculate the perc_rep for the dropout dataset \n",
    "                ### NOTE: in utilsphering, calculate_percent_replicating_MOA has been updated to return\n",
    "                ### replicate_corr, null_corr, prop_95, value_95_replicating\n",
    "                ### In this example, calculate_percent_replicating_MOA is **only returning prop_95**\n",
    "                dropout_dict[f'BR001202{eachplate}']= calculate_percent_replicating_MOA('','',data_df=feature_select)\n",
    "            except:\n",
    "                dropout_dict[f'BR001202{eachplate}']=0\n",
    "        # Convert the calculated perc_rep into np.array for each dropout \n",
    "        results_array = np.array(list(dropout_dict.values()))\n",
    "        for k,v in temp_dropout_dict.items():\n",
    "            dropout_dict[k]=v\n",
    "        # For the dropout dataset created, calculate the mean perc_replicating for ALL of the plates\n",
    "        # Why all of the plates? Are these replicates?\n",
    "        # Means are calculated within each dropout group\n",
    "        dropout_dict['Mean']=results_array.mean()\n",
    "        dropout_dict['Median']=np.median(results_array)\n",
    "        dropout_dict['Standard deviation']=np.std(results_array)\n",
    "        dropout_dict['Dropout']=eachdropout\n",
    "        dropout_dict['Remaining'] = [x for x in category_list if x not in eachdropout]\n",
    "        dropout_dict['n_columns']=len(col_list)\n",
    "        dropout_df = dropout_df.append(dropout_dict,ignore_index=True)\n",
    "        print(eachdropout,dropout_dict['Mean'])\n",
    "        dropout_df.to_csv(outfile,index=False)\n",
    "    print(dropout_df['Mean'].describe())\n",
    "    print(dropout_df['Median'].describe())\n",
    "\n",
    "def category_dropout(\n",
    "    dataframe, \n",
    "    plate_barcode, \n",
    "    category_list, \n",
    "    match_or_rep_or_both=\"replicating\",\n",
    "    enable_sphering=\"both\", \n",
    "    outfile=None\n",
    "):\n",
    "    input_columns = dataframe.columns\n",
    "    possible_combinations = [y for x in range(len(category_list)+1) for y in list(set(itertools.combinations(category_list,x)))]\n",
    "    rep_df_list = list()\n",
    "    match_df_list = list()\n",
    "\n",
    "    # Check if dropout has previously been performed\n",
    "    # if os.path.exists(outfile):\n",
    "    #     # Limitation of this implementation:\n",
    "    #     # If you switch from sphering to not sphering, this will fail to \n",
    "    #     # run the analysis without sphering as it only checks the 'dropout'\n",
    "    #     # column combinations and not any other column (such as if sphering was T/F).\n",
    "    #     dropout_df = pd.read_csv(outfile)\n",
    "    #     rep_df_list.append(dropout_df)\n",
    "    #     match_df_list.append(dropout_df)\n",
    "    #     tried=list(dropout_df[\"dropout\"])\n",
    "    #     print(f\"Already tried {tried}. Skipping.\")\n",
    "    #     possible_combinations = [x for x in possible_combinations if str(x) not in tried]\n",
    "    #     # Otherwise, dropout columns will be generated from scratch\n",
    "\n",
    "    if len(possible_combinations) == 0:\n",
    "        # Either no combinations available or they're already present in the outfile\n",
    "        print(\"All combinations already computed\")\n",
    "        return\n",
    "\n",
    "    # Iterate through the combination to dropout\n",
    "    for dropout_group in possible_combinations: \n",
    "        col_list = input_columns\n",
    "        # Within the droupout group, find the column to actually drop\n",
    "        for each_item in dropout_group:\n",
    "            # Only keep columns that don't contain the dropout\n",
    "            col_list = [x for x in col_list if each_item not in x]\n",
    "        print(f\"---- Dropping: {dropout_group} ----\")\n",
    "        try:\n",
    "            dropped_dataframe = pd.DataFrame(dataframe[col_list])\n",
    "            feature_select = do_feature_select(dropped_dataframe)\n",
    "        except:\n",
    "            feature_select = dropped_dataframe\n",
    "        # return feature_select\n",
    "        if match_or_rep_or_both.casefold() == \"replicating\" or match_or_rep_or_both.casefold() == \"both\":\n",
    "            if enable_sphering.casefold() == \"yes\" or enable_sphering.casefold() == \"both\":\n",
    "                try:\n",
    "                    # Added to tackle an issue where longer dropout runs would error out due to NaN input\n",
    "                    # Unsure of the source of the NaN input. This is a temporary solution.\n",
    "                    prop_95 = return_prop95(feature_select, match_or_rep=\"replicating\", enable_sphering=True)\n",
    "                except:\n",
    "                    prop_95 = 0\n",
    "                _df = pd.DataFrame({\n",
    "                    \"assay_plate_barcode\": plate_barcode,\n",
    "                    \"num_features\": len(feature_select.columns),\n",
    "                    \"dropout\": str(dropout_group),\n",
    "                    \"n_columns\": len(col_list),\n",
    "                    \"sphering\": True,\n",
    "                    \"percent_replicating\": [prop_95]\n",
    "                })\n",
    "                rep_df_list.append(_df)\n",
    "            if enable_sphering.casefold() == \"no\" or enable_sphering.casefold() == \"both\":\n",
    "                try:\n",
    "                    prop_95 = return_prop95(feature_select, match_or_rep=\"replicating\", enable_sphering=False)\n",
    "                except:\n",
    "                    prop_95 = 0                \n",
    "                _df = pd.DataFrame({\n",
    "                    \"assay_plate_barcode\": plate_barcode,\n",
    "                    \"num_features\": len(feature_select.columns),\n",
    "                    \"dropout\": str(dropout_group),\n",
    "                    \"n_columns\": len(col_list),\n",
    "                    \"sphering\": False,\n",
    "                    \"percent_replicating\": [prop_95]\n",
    "                })\n",
    "                rep_df_list.append(_df)\n",
    "        if match_or_rep_or_both.casefold() == \"matching\" or match_or_rep_or_both.casefold() == \"both\":\n",
    "            if enable_sphering.casefold() == \"yes\" or enable_sphering.casefold() == \"both\":\n",
    "                try:\n",
    "                    prop_95 = return_prop95(feature_select, match_or_rep=\"matching\", enable_sphering=True)\n",
    "                except:\n",
    "                    prop_95 = 0\n",
    "                _df = pd.DataFrame({\n",
    "                    \"assay_plate_barcode\": plate_barcode,\n",
    "                    \"num_features\": len(feature_select.columns),\n",
    "                    \"dropout\": str(dropout_group),\n",
    "                    \"n_columns\": len(col_list),\n",
    "                    \"sphering\": True,\n",
    "                    \"percent_matching\": [prop_95]\n",
    "                })\n",
    "                match_df_list.append(_df)\n",
    "            if enable_sphering.casefold() == \"no\" or enable_sphering.casefold() == \"both\":\n",
    "                try:\n",
    "                    prop_95 = return_prop95(feature_select, match_or_rep=\"matching\", enable_sphering=False)\n",
    "                except:\n",
    "                    prop_95 = 0\n",
    "                _df = pd.DataFrame({\n",
    "                    \"assay_plate_barcode\": plate_barcode,\n",
    "                    \"num_features\": len(feature_select.columns),\n",
    "                    \"dropout\": str(dropout_group),\n",
    "                    \"n_columns\": len(col_list),\n",
    "                    \"sphering\": False,\n",
    "                    \"percent_matching\": [prop_95],\n",
    "                })\n",
    "                match_df_list.append(_df)\n",
    "        \n",
    "    # Concatenate the data\n",
    "    if match_or_rep_or_both.casefold() == \"replicating\" or match_or_rep_or_both.casefold() == \"both\":\n",
    "        rep_df = pd.concat(rep_df_list, ignore_index=True)\n",
    "    if match_or_rep_or_both.casefold() == \"matching\" or match_or_rep_or_both.casefold() == \"both\":\n",
    "        match_df = pd.concat(match_df_list, ignore_index=True)\n",
    "                \n",
    "    # Merge so perc rep and match is in the same place\n",
    "    if match_or_rep_or_both.casefold() == \"both\":\n",
    "        merged_df = rep_df.merge(match_df, how=\"inner\")\n",
    "        if outfile is not None:\n",
    "            merged_df.to_csv(outfile,index=False)\n",
    "        return merged_df\n",
    "    if match_or_rep_or_both.casefold() == \"replicating\":\n",
    "        if outfile is not None:\n",
    "            rep_df.to_csv(outfile,index=False)\n",
    "        return rep_df\n",
    "    if match_or_rep_or_both.casefold() == \"matching\":\n",
    "        if outfile is not None:\n",
    "            match_df.to_csv(outfile,index=False)\n",
    "        return match_df\n",
    "        \n",
    "# TODO: Check that this is accurately dropping, integrate with os.walk, integrate with heatmap matrix\n",
    "# No need to integrate with os.walk. Use metadata csv\n",
    "\n",
    "\n",
    "\n",
    "load_file = \"../jump-scope/profiles/1siteSubSample_Scope1_MolDev_10X/Plate2_PCO_6ch_4site_10XPA/Plate2_PCO_6ch_4site_10XPA_normalized_negcon.csv.gz\"\n",
    "test_data = pd.read_csv(load_file)\n",
    "drops = category_dropout(test_data, \"Plate2_PCO_6ch_4site_10XPA\", ['Nuclei', 'Cells', 'Cytoplasm'], \"both\", \"both\", \"test.csv\")\n",
    "drops\n",
    "# ['Nuclei','Cells','Cytoplasm']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gather the dropout data\n",
    "df_list = list()\n",
    "t = pd.DataFrame()\n",
    "_, parent_folders, _ = next(os.walk(\"../jump-scope/profiles/\"))\n",
    "for batch_name in parent_folders:\n",
    "    _, subdirs, _ = next(os.walk(f\"../jump-scope/profiles/{batch_name}\"))\n",
    "    for plate_barcode in subdirs:\n",
    "        batch_path = f\"../jump-scope/profiles/{batch_name}/{plate_barcode}\"\n",
    "        for _, _, files in os.walk(batch_path):\n",
    "            for filename in files:\n",
    "                if filename.endswith(\"-dropout.csv\"):\n",
    "                    dropout_path = f\"{batch_path}/{plate_barcode}-dropout.csv\"\n",
    "                    dropout_df = pd.read_csv(dropout_path)\n",
    "                    dropout_df[\"batch_name\"] = batch_name\n",
    "                    df_list.append(dropout_df)\n",
    "\n",
    "collated_dropout_df = pd.concat(df_list)\n",
    "collated_dropout_df.to_csv(\"checkpoints/feature-dropouts-nuclei_cytoplasm_cells.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_and_graph_heatmap(df,col_list,row_list,cmap,title,figsize,xbreaklen=120,ybreaklen=180,xkcd=False):\n",
    "    full_col_list = []\n",
    "    full_col_list_for_labels = []\n",
    "    full_row_list= []\n",
    "    full_row_list_for_labels = []\n",
    "    for eachlen in range(len(col_list)+1):\n",
    "        full_col_list += sorted([str(sorted(x)) for x in list(itertools.combinations(col_list,eachlen))])\n",
    "        full_col_list_for_labels += sorted([(' + ').join(sorted(x)) for x in list(itertools.combinations(col_list,eachlen))])\n",
    "    for eachlen in range(len(row_list)+1):\n",
    "        full_row_list += sorted([str(sorted(x)) for x in list(itertools.combinations(row_list,eachlen))])\n",
    "        full_row_list_for_labels += sorted([(' + ').join(sorted(x)) for x in list(itertools.combinations(row_list,eachlen))])\n",
    "    square=pd.DataFrame(columns=full_col_list,index=full_row_list)\n",
    "    for _,row in df.iterrows():\n",
    "        sub_rows=[]\n",
    "        sub_cols=[]\n",
    "        for thing in row['dropout']:\n",
    "            if thing not in col_list:\n",
    "                sub_cols.append(thing)\n",
    "            else:\n",
    "                sub_rows.append(thing)\n",
    "        sub_rows.sort()\n",
    "        sub_cols.sort()\n",
    "        # square[str(sub_cols)][str(sub_rows)]=row['percent_replicating']*100\n",
    "        square.loc[str(sub_cols),str(sub_rows)] = row[\"percent_replicating\"]\n",
    "    square=square.fillna(0)\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    fig.set_facecolor(\"white\")\n",
    "    # if xkcd:\n",
    "    #     cmap=sns.light_palette(cmap,input='xkcd',as_cmap=True)\n",
    "    g=sns.heatmap(square,cmap=cmap,annot=True)\n",
    "    xlabels=['None']\n",
    "    ax.xaxis.tick_top()\n",
    "    ax.xaxis.set_label_position('top')\n",
    "    ax.tick_params(length=0)\n",
    "    for label in full_col_list_for_labels[1:]:\n",
    "        xlabels.append(textwrap.fill(label, width=xbreaklen/len(full_col_list_for_labels),break_long_words=False))\n",
    "    g.set_xticklabels(xlabels, rotation=0)\n",
    "    ylabels=['None']\n",
    "    for label in full_row_list_for_labels[1:]:\n",
    "        ylabels.append(textwrap.fill(label, width=ybreaklen/len(full_row_list_for_labels),break_long_words=False))\n",
    "    g.set_yticklabels(ylabels, rotation=0)\n",
    "    g.set_title(title)\n",
    "    # plt.savefig(f'figures/{title}.png',dpi=300)\n",
    "    \n",
    "    \n",
    "create_and_graph_heatmap(\n",
    "    dropout_df,\n",
    "    col_list=['Nuclei','Cytoplasm'],\n",
    "    row_list=['Cells'],\n",
    "    cmap='BuPu',\n",
    "    title = 'Mean Percent Replicating by Compartments Present for ALL profiles',\n",
    "    figsize=(10,6))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropout groups\n",
    "# Basically, make matrix plots for profile groups\n",
    "# ie. compare dropouts for the same batch that have different site subsamples\n",
    "\n",
    "subsample_df = pd.read_csv('output/FoV-experiment-metadata.tsv', sep='\\t')\n",
    "\n",
    "dropout_df = pd.read_csv(\n",
    "    \"checkpoints/feature-dropouts-nuclei_cytoplasm_cells.csv\", \n",
    "    converters={'dropout':ast.literal_eval}\n",
    ")\n",
    "dropout_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new DF that combines the dropout results with the subsample info\n",
    "\n",
    "batch_list = list(set(dropout_df.loc[:,\"batch_name\"]))\n",
    "plate_list = list(set(dropout_df.loc[:, \"assay_plate_barcode\"]))\n",
    "for plate in plate_list:\n",
    "    if plate == \"BRO0117033_20xb\":\n",
    "    # print(plate)\n",
    "        t = dropout_df.loc[dropout_df[\"assay_plate_barcode\"] == plate]\n",
    "        for subset in set(t[\"batch_name\"]):\n",
    "            subset_df = t[t[\"batch_name\"] == subset]\n",
    "            create_and_graph_heatmap(\n",
    "                subset_df,\n",
    "                col_list=['Nuclei','Cytoplasm'],\n",
    "                row_list=['Cells'],\n",
    "                cmap='BuPu',\n",
    "                title = f\"Percent replicating for {subset}\",\n",
    "                figsize=(10,6))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alt \n",
    "\n",
    "def create_and_graph_heatmap(df,col_list,row_list,ax_=None,xbreaklen=120,ybreaklen=180):\n",
    "    full_col_list = []\n",
    "    full_col_list_for_labels = []\n",
    "    full_row_list= []\n",
    "    full_row_list_for_labels = []\n",
    "    for eachlen in range(len(col_list)+1):\n",
    "        # This works out all of the combinations for the given col list (eg. nuclei, nuclei+cells...)\n",
    "        # Each combination is sorted so the label order is consistent, then the whole list itself is sorted\n",
    "        # itertools.combinations will only return combinations that are equal in length to each len,\n",
    "        # thus only combination lengths of 1 and 2 will be returned for col_list=[\"Nuclei\", \"Cells\"]\n",
    "        full_col_list += sorted([str(sorted(x)) for x in list(itertools.combinations(col_list,eachlen))])\n",
    "        # This generates the actual labels for above, ready to be displayed\n",
    "        full_col_list_for_labels += sorted([(' + ').join(sorted(x)) for x in list(itertools.combinations(col_list,eachlen))])\n",
    "    for eachlen in range(len(row_list)+1):\n",
    "        full_row_list += sorted([str(sorted(x)) for x in list(itertools.combinations(row_list,eachlen))])\n",
    "        full_row_list_for_labels += sorted([(' + ').join(sorted(x)) for x in list(itertools.combinations(row_list,eachlen))])\n",
    "    square=pd.DataFrame(columns=full_col_list,index=full_row_list)\n",
    "    for _,row in df.iterrows():\n",
    "        sub_rows=[]\n",
    "        sub_cols=[]\n",
    "        for thing in row['dropout']:\n",
    "            if thing not in col_list:\n",
    "                sub_cols.append(thing)\n",
    "            else:\n",
    "                sub_rows.append(thing)\n",
    "        sub_rows.sort()\n",
    "        sub_cols.sort()\n",
    "        square.loc[str(sub_cols),str(sub_rows)] = row[\"percent_replicating\"]\n",
    "    square=square.fillna(0)\n",
    "    if ax_ is None:\n",
    "        fig, ax = plt.subplots(figsize=(15,15))\n",
    "    fig.set_facecolor(\"white\")\n",
    "    g=sns.heatmap(square,annot=True)\n",
    "    xlabels=['None']\n",
    "    ax.xaxis.tick_top()\n",
    "    ax.xaxis.set_label_position('top')\n",
    "    ax.tick_params(length=0)\n",
    "    for label in full_col_list_for_labels[1:]:\n",
    "        xlabels.append(textwrap.fill(label, width=xbreaklen/len(full_col_list_for_labels),break_long_words=False))\n",
    "    g.set_xticklabels(xlabels, rotation=0)\n",
    "    ylabels=['None']\n",
    "    for label in full_row_list_for_labels[1:]:\n",
    "        ylabels.append(textwrap.fill(label, width=ybreaklen/len(full_row_list_for_labels),break_long_words=False))\n",
    "    g.set_yticklabels(ylabels, rotation=0)\n",
    "    g.set_title(title)\n",
    "    # plt.savefig(f'figures/{title}.png',dpi=300)\n",
    "    \n",
    "    \n",
    "create_and_graph_heatmap(\n",
    "    dropout_df,\n",
    "    col_list=['Nuclei','Cytoplasm'],\n",
    "    row_list=['Cells'],\n",
    "    cmap='BuPu',\n",
    "    title = 'Mean Percent Replicating by Compartments Present for ALL profiles',\n",
    "    figsize=(10,6))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_and_graph_heatmap(df,col_list,row_list,ax_=None,xbreaklen=120,ybreaklen=180):\n",
    "    full_col_list = []\n",
    "    full_col_list_for_labels = []\n",
    "    full_row_list= []\n",
    "    full_row_list_for_labels = []\n",
    "    for eachlen in range(len(col_list)+1):\n",
    "        full_col_list = sorted([y for x in range(len(col_list)+1) for y in list(set(itertools.combinations(col_list,x)))])\n",
    "        full_col_list += sorted([str(sorted(x)) for x in list(itertools.combinations(col_list,eachlen))])\n",
    "        # This generates the actual labels for above, ready to be displayed\n",
    "        sorted([y for x in range(len(col_list)+1) for y in list(set(itertools.combinations(col_list,x)))])\n",
    "        full_col_list_for_labels += sorted([(' + ').join(sorted(x)) for x in list(itertools.combinations(col_list,eachlen))])\n",
    "    for eachlen in range(len(row_list)+1):\n",
    "        full_row_list += sorted([str(sorted(x)) for x in list(itertools.combinations(row_list,eachlen))])\n",
    "        full_row_list_for_labels += sorted([(' + ').join(sorted(x)) for x in list(itertools.combinations(row_list,eachlen))])\n",
    "    square=pd.DataFrame(columns=full_col_list,index=full_row_list)\n",
    "    for _,row in df.iterrows():\n",
    "        sub_rows=[]\n",
    "        sub_cols=[]\n",
    "        for thing in row['dropout']:\n",
    "            if thing not in col_list:\n",
    "                sub_cols.append(thing)\n",
    "            else:\n",
    "                sub_rows.append(thing)\n",
    "        sub_rows.sort()\n",
    "        sub_cols.sort()\n",
    "        square.loc[str(sub_cols),str(sub_rows)] = row[\"percent_replicating\"]\n",
    "    square=square.fillna(0)\n",
    "    if ax_ is None:\n",
    "        fig, ax = plt.subplots(figsize=(15,15))\n",
    "    fig.set_facecolor(\"white\")\n",
    "    g=sns.heatmap(square,annot=True)\n",
    "    xlabels=['None']\n",
    "    ax.xaxis.tick_top()\n",
    "    ax.xaxis.set_label_position('top')\n",
    "    ax.tick_params(length=0)\n",
    "    for label in full_col_list_for_labels[1:]:\n",
    "        xlabels.append(textwrap.fill(label, width=xbreaklen/len(full_col_list_for_labels),break_long_words=False))\n",
    "    g.set_xticklabels(xlabels, rotation=0)\n",
    "    ylabels=['None']\n",
    "    for label in full_row_list_for_labels[1:]:\n",
    "        ylabels.append(textwrap.fill(label, width=ybreaklen/len(full_row_list_for_labels),break_long_words=False))\n",
    "    g.set_yticklabels(ylabels, rotation=0)\n",
    "    g.set_title(title)\n",
    "    # plt.savefig(f'figures/{title}.png',dpi=300)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "90bd059e05f79fb9b7cf5d2b1dae6ea26ca779772e058f49dd8fbe1978749df0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
